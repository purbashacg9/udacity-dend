{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Title\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "The purpose of the project is to build an ETL pipeline that extracts data from multiple data sets and transforms them into analytical tables containing information on immigration to United States. The analytical tables are stored in a data lake on S3. For purpose of this workspace, the data is being stored in the output folder in this workspace. The output folder can be imagined as a replacement for a S3 bucket. \n",
    "\n",
    "The project follows the steps outlined below:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "The goal of the project is to build a datalake on Amazon S3 containing analytical tables. This project uses Udacity provides datasets and an external dataset on airline routes. \n",
    "\n",
    "Datasets - \n",
    "1. Immigration data from 2016 \n",
    "2. Demographics of US cities\n",
    "3. Airport data for airports acros  \n",
    "4. Temperature data \n",
    "5. Airline Routes\n",
    "\n",
    "Analyze immigration data and build a data lake containing aggregated data  that provides information on the people immigrating in and out of US, what countries they are coming from, which cities they are settling in, what are the popular point of entries. etc. \n",
    " \n",
    "The end solution is a data lake  containing the following tables - \n",
    "1. immigration_data_with_demographics - Demographics data grouped by states is joined with immigration data for comparing how immigration impacts a state's demographics. \n",
    "\n",
    "\n",
    "Tools used for this project are - \n",
    "1. Jupyter Notebooks \n",
    "2. pandas library for preliminary data analysis\n",
    "3. Apache Spark for data processing and transformation of the data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Describe and Gather Data \n",
    "The sections below I used the pandas library for exploring the datasets and looking at the data samples. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Immigration Data for 2016\n",
    "This data comes from the US National Tourism and Trade Office. The dataset is composed of files in SASB7DAT format. There's a file for each month of the year. The files in the project workspace in the data/18-83510-I94-Data-2016 folder. \n",
    "The data dictionary for this dataset is provided in the file I94_SAS_Labels_Descriptions.SAS. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta, date\n",
    "import os\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import LongType, IntegerType, StringType\n",
    "from pyspark.sql.functions import col, udf, year, date_format, sum, avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's example data from one file \n",
    "immi_df = pd.read_sas('file://localhost/data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat', \n",
    "                          format='sas7bdat', \n",
    "                          index=None,\n",
    "                          encoding='ISO-8859-1', \n",
    "                          chunksize=None, \n",
    "                          iterator=False\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cicid</th>\n",
       "      <th>i94yr</th>\n",
       "      <th>i94mon</th>\n",
       "      <th>i94cit</th>\n",
       "      <th>i94res</th>\n",
       "      <th>i94port</th>\n",
       "      <th>arrdate</th>\n",
       "      <th>i94mode</th>\n",
       "      <th>i94addr</th>\n",
       "      <th>depdate</th>\n",
       "      <th>...</th>\n",
       "      <th>entdepu</th>\n",
       "      <th>matflag</th>\n",
       "      <th>biryear</th>\n",
       "      <th>dtaddto</th>\n",
       "      <th>gender</th>\n",
       "      <th>insnum</th>\n",
       "      <th>airline</th>\n",
       "      <th>admnum</th>\n",
       "      <th>fltno</th>\n",
       "      <th>visatype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>692.0</td>\n",
       "      <td>692.0</td>\n",
       "      <td>XXX</td>\n",
       "      <td>20573.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>U</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1979.0</td>\n",
       "      <td>10282016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.897628e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>254.0</td>\n",
       "      <td>276.0</td>\n",
       "      <td>ATL</td>\n",
       "      <td>20551.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>AL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Y</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1991.0</td>\n",
       "      <td>D/S</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.736796e+09</td>\n",
       "      <td>00296</td>\n",
       "      <td>F1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>WAS</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MI</td>\n",
       "      <td>20691.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1961.0</td>\n",
       "      <td>09302016</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>OS</td>\n",
       "      <td>6.666432e+08</td>\n",
       "      <td>93</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>NYC</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MA</td>\n",
       "      <td>20567.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1988.0</td>\n",
       "      <td>09302016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AA</td>\n",
       "      <td>9.246846e+10</td>\n",
       "      <td>00199</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>NYC</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MA</td>\n",
       "      <td>20567.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>09302016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AA</td>\n",
       "      <td>9.246846e+10</td>\n",
       "      <td>00199</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   cicid   i94yr  i94mon  i94cit  i94res i94port  arrdate  i94mode i94addr  \\\n",
       "0    6.0  2016.0     4.0   692.0   692.0     XXX  20573.0      NaN     NaN   \n",
       "1    7.0  2016.0     4.0   254.0   276.0     ATL  20551.0      1.0      AL   \n",
       "2   15.0  2016.0     4.0   101.0   101.0     WAS  20545.0      1.0      MI   \n",
       "3   16.0  2016.0     4.0   101.0   101.0     NYC  20545.0      1.0      MA   \n",
       "4   17.0  2016.0     4.0   101.0   101.0     NYC  20545.0      1.0      MA   \n",
       "\n",
       "   depdate   ...     entdepu  matflag  biryear   dtaddto gender insnum  \\\n",
       "0      NaN   ...           U      NaN   1979.0  10282016    NaN    NaN   \n",
       "1      NaN   ...           Y      NaN   1991.0       D/S      M    NaN   \n",
       "2  20691.0   ...         NaN        M   1961.0  09302016      M    NaN   \n",
       "3  20567.0   ...         NaN        M   1988.0  09302016    NaN    NaN   \n",
       "4  20567.0   ...         NaN        M   2012.0  09302016    NaN    NaN   \n",
       "\n",
       "  airline        admnum  fltno visatype  \n",
       "0     NaN  1.897628e+09    NaN       B2  \n",
       "1     NaN  3.736796e+09  00296       F1  \n",
       "2      OS  6.666432e+08     93       B2  \n",
       "3      AA  9.246846e+10  00199       B2  \n",
       "4      AA  9.246846e+10  00199       B2  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "immi_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3096313, 28)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "immi_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### World Temperature Data \n",
    "This dataset comes from Kaggle datasets. It is available at https://www.kaggle.com/berkeleyearth/climate-change-earth-surface-temperature-data. The dataset is provided in the project workspace as GlobalLandTemperaturesByCity.csv. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_data_fname = '../../data2/GlobalLandTemperaturesByCity.csv'\n",
    "weather_df = pd.read_csv(weather_data_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8599212 entries, 0 to 8599211\n",
      "Data columns (total 7 columns):\n",
      "dt                               object\n",
      "AverageTemperature               float64\n",
      "AverageTemperatureUncertainty    float64\n",
      "City                             object\n",
      "Country                          object\n",
      "Latitude                         object\n",
      "Longitude                        object\n",
      "dtypes: float64(2), object(5)\n",
      "memory usage: 459.2+ MB\n"
     ]
    }
   ],
   "source": [
    "weather_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dt</th>\n",
       "      <th>AverageTemperature</th>\n",
       "      <th>AverageTemperatureUncertainty</th>\n",
       "      <th>City</th>\n",
       "      <th>Country</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1743-11-01</td>\n",
       "      <td>6.068</td>\n",
       "      <td>1.737</td>\n",
       "      <td>Ã…rhus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1743-12-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ã…rhus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1744-01-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ã…rhus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1744-02-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ã…rhus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1744-03-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ã…rhus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           dt  AverageTemperature  AverageTemperatureUncertainty   City  \\\n",
       "0  1743-11-01               6.068                          1.737  Ã…rhus   \n",
       "1  1743-12-01                 NaN                            NaN  Ã…rhus   \n",
       "2  1744-01-01                 NaN                            NaN  Ã…rhus   \n",
       "3  1744-02-01                 NaN                            NaN  Ã…rhus   \n",
       "4  1744-03-01                 NaN                            NaN  Ã…rhus   \n",
       "\n",
       "   Country Latitude Longitude  \n",
       "0  Denmark   57.05N    10.33E  \n",
       "1  Denmark   57.05N    10.33E  \n",
       "2  Denmark   57.05N    10.33E  \n",
       "3  Denmark   57.05N    10.33E  \n",
       "4  Denmark   57.05N    10.33E  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Airport Codes\n",
    "This is a dataset of airport codes (IATA and GPS codes) from across the globe. It also includes name and type of the airport and the region and municipality in which it is located. \n",
    "It comes from https://datahub.io/core/airport-codes#data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "airport_codes_fname = 'airport-codes_csv.csv'\n",
    "airport_codes_df = pd.read_csv(airport_codes_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 55075 entries, 0 to 55074\n",
      "Data columns (total 12 columns):\n",
      "ident           55075 non-null object\n",
      "type            55075 non-null object\n",
      "name            55075 non-null object\n",
      "elevation_ft    48069 non-null float64\n",
      "continent       27356 non-null object\n",
      "iso_country     54828 non-null object\n",
      "iso_region      55075 non-null object\n",
      "municipality    49399 non-null object\n",
      "gps_code        41030 non-null object\n",
      "iata_code       9189 non-null object\n",
      "local_code      28686 non-null object\n",
      "coordinates     55075 non-null object\n",
      "dtypes: float64(1), object(11)\n",
      "memory usage: 5.0+ MB\n"
     ]
    }
   ],
   "source": [
    "airport_codes_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ident</th>\n",
       "      <th>type</th>\n",
       "      <th>name</th>\n",
       "      <th>elevation_ft</th>\n",
       "      <th>continent</th>\n",
       "      <th>iso_country</th>\n",
       "      <th>iso_region</th>\n",
       "      <th>municipality</th>\n",
       "      <th>gps_code</th>\n",
       "      <th>iata_code</th>\n",
       "      <th>local_code</th>\n",
       "      <th>coordinates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00A</td>\n",
       "      <td>heliport</td>\n",
       "      <td>Total Rf Heliport</td>\n",
       "      <td>11.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-PA</td>\n",
       "      <td>Bensalem</td>\n",
       "      <td>00A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00A</td>\n",
       "      <td>-74.93360137939453, 40.07080078125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00AA</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Aero B Ranch Airport</td>\n",
       "      <td>3435.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-KS</td>\n",
       "      <td>Leoti</td>\n",
       "      <td>00AA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00AA</td>\n",
       "      <td>-101.473911, 38.704022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00AK</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Lowell Field</td>\n",
       "      <td>450.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AK</td>\n",
       "      <td>Anchor Point</td>\n",
       "      <td>00AK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00AK</td>\n",
       "      <td>-151.695999146, 59.94919968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00AL</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Epps Airpark</td>\n",
       "      <td>820.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AL</td>\n",
       "      <td>Harvest</td>\n",
       "      <td>00AL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00AL</td>\n",
       "      <td>-86.77030181884766, 34.86479949951172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00AR</td>\n",
       "      <td>closed</td>\n",
       "      <td>Newport Hospital &amp; Clinic Heliport</td>\n",
       "      <td>237.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AR</td>\n",
       "      <td>Newport</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-91.254898, 35.6087</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ident           type                                name  elevation_ft  \\\n",
       "0   00A       heliport                   Total Rf Heliport          11.0   \n",
       "1  00AA  small_airport                Aero B Ranch Airport        3435.0   \n",
       "2  00AK  small_airport                        Lowell Field         450.0   \n",
       "3  00AL  small_airport                        Epps Airpark         820.0   \n",
       "4  00AR         closed  Newport Hospital & Clinic Heliport         237.0   \n",
       "\n",
       "  continent iso_country iso_region  municipality gps_code iata_code  \\\n",
       "0       NaN          US      US-PA      Bensalem      00A       NaN   \n",
       "1       NaN          US      US-KS         Leoti     00AA       NaN   \n",
       "2       NaN          US      US-AK  Anchor Point     00AK       NaN   \n",
       "3       NaN          US      US-AL       Harvest     00AL       NaN   \n",
       "4       NaN          US      US-AR       Newport      NaN       NaN   \n",
       "\n",
       "  local_code                            coordinates  \n",
       "0        00A     -74.93360137939453, 40.07080078125  \n",
       "1       00AA                 -101.473911, 38.704022  \n",
       "2       00AK            -151.695999146, 59.94919968  \n",
       "3       00AL  -86.77030181884766, 34.86479949951172  \n",
       "4        NaN                    -91.254898, 35.6087  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airport_codes_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Demographics Data for US Cities \n",
    "This dataset contains information about the demographics of all US cities and census-designated places with a population greater or equal to 65,000. This data comes from the US Census Bureau's 2015 American Community Survey. \n",
    "The dataset is available at https://public.opendatasoft.com/explore/dataset/us-cities-demographics/information/. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "demographics_fname = 'us-cities-demographics.csv'\n",
    "demographics_df = pd.read_csv(demographics_fname, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2891 entries, 0 to 2890\n",
      "Data columns (total 12 columns):\n",
      "City                      2891 non-null object\n",
      "State                     2891 non-null object\n",
      "Median Age                2891 non-null float64\n",
      "Male Population           2888 non-null float64\n",
      "Female Population         2888 non-null float64\n",
      "Total Population          2891 non-null int64\n",
      "Number of Veterans        2878 non-null float64\n",
      "Foreign-born              2878 non-null float64\n",
      "Average Household Size    2875 non-null float64\n",
      "State Code                2891 non-null object\n",
      "Race                      2891 non-null object\n",
      "Count                     2891 non-null int64\n",
      "dtypes: float64(6), int64(2), object(4)\n",
      "memory usage: 271.1+ KB\n"
     ]
    }
   ],
   "source": [
    "demographics_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Median Age</th>\n",
       "      <th>Male Population</th>\n",
       "      <th>Female Population</th>\n",
       "      <th>Total Population</th>\n",
       "      <th>Number of Veterans</th>\n",
       "      <th>Foreign-born</th>\n",
       "      <th>Average Household Size</th>\n",
       "      <th>State Code</th>\n",
       "      <th>Race</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Silver Spring</td>\n",
       "      <td>Maryland</td>\n",
       "      <td>33.8</td>\n",
       "      <td>40601.0</td>\n",
       "      <td>41862.0</td>\n",
       "      <td>82463</td>\n",
       "      <td>1562.0</td>\n",
       "      <td>30908.0</td>\n",
       "      <td>2.60</td>\n",
       "      <td>MD</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>25924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Quincy</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>41.0</td>\n",
       "      <td>44129.0</td>\n",
       "      <td>49500.0</td>\n",
       "      <td>93629</td>\n",
       "      <td>4147.0</td>\n",
       "      <td>32935.0</td>\n",
       "      <td>2.39</td>\n",
       "      <td>MA</td>\n",
       "      <td>White</td>\n",
       "      <td>58723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hoover</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>38.5</td>\n",
       "      <td>38040.0</td>\n",
       "      <td>46799.0</td>\n",
       "      <td>84839</td>\n",
       "      <td>4819.0</td>\n",
       "      <td>8229.0</td>\n",
       "      <td>2.58</td>\n",
       "      <td>AL</td>\n",
       "      <td>Asian</td>\n",
       "      <td>4759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rancho Cucamonga</td>\n",
       "      <td>California</td>\n",
       "      <td>34.5</td>\n",
       "      <td>88127.0</td>\n",
       "      <td>87105.0</td>\n",
       "      <td>175232</td>\n",
       "      <td>5821.0</td>\n",
       "      <td>33878.0</td>\n",
       "      <td>3.18</td>\n",
       "      <td>CA</td>\n",
       "      <td>Black or African-American</td>\n",
       "      <td>24437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Newark</td>\n",
       "      <td>New Jersey</td>\n",
       "      <td>34.6</td>\n",
       "      <td>138040.0</td>\n",
       "      <td>143873.0</td>\n",
       "      <td>281913</td>\n",
       "      <td>5829.0</td>\n",
       "      <td>86253.0</td>\n",
       "      <td>2.73</td>\n",
       "      <td>NJ</td>\n",
       "      <td>White</td>\n",
       "      <td>76402</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               City          State  Median Age  Male Population  \\\n",
       "0     Silver Spring       Maryland        33.8          40601.0   \n",
       "1            Quincy  Massachusetts        41.0          44129.0   \n",
       "2            Hoover        Alabama        38.5          38040.0   \n",
       "3  Rancho Cucamonga     California        34.5          88127.0   \n",
       "4            Newark     New Jersey        34.6         138040.0   \n",
       "\n",
       "   Female Population  Total Population  Number of Veterans  Foreign-born  \\\n",
       "0            41862.0             82463              1562.0       30908.0   \n",
       "1            49500.0             93629              4147.0       32935.0   \n",
       "2            46799.0             84839              4819.0        8229.0   \n",
       "3            87105.0            175232              5821.0       33878.0   \n",
       "4           143873.0            281913              5829.0       86253.0   \n",
       "\n",
       "   Average Household Size State Code                       Race  Count  \n",
       "0                    2.60         MD         Hispanic or Latino  25924  \n",
       "1                    2.39         MA                      White  58723  \n",
       "2                    2.58         AL                      Asian   4759  \n",
       "3                    3.18         CA  Black or African-American  24437  \n",
       "4                    2.73         NJ                      White  76402  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demographics_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Flight Route Database\n",
    "A database of 59,036 flight routes. As of January 2012, the OpenFlights/Airline Route Mapper Route Database contains 59036 routes between 3209 airports on 531 airlines spanning the globe. \n",
    "Source of dataset - https://www.kaggle.com/open-flights/flight-route-database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "flight_routes_fname = 'routes.csv'\n",
    "flight_routes_df = pd.read_csv(flight_routes_fname, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 67663 entries, 0 to 67662\n",
      "Data columns (total 9 columns):\n",
      "airline                   67663 non-null object\n",
      "airline_id                67663 non-null object\n",
      "source_airport            67663 non-null object\n",
      "source_airport_id         67663 non-null object\n",
      "destination_airport       67663 non-null object\n",
      "destination_airport_id    67663 non-null object\n",
      "codeshare                 14597 non-null object\n",
      "stops                     67663 non-null int64\n",
      "equipment                 67645 non-null object\n",
      "dtypes: int64(1), object(8)\n",
      "memory usage: 4.6+ MB\n"
     ]
    }
   ],
   "source": [
    "flight_routes_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline</th>\n",
       "      <th>airline_id</th>\n",
       "      <th>source_airport</th>\n",
       "      <th>source_airport_id</th>\n",
       "      <th>destination_airport</th>\n",
       "      <th>destination_airport_id</th>\n",
       "      <th>codeshare</th>\n",
       "      <th>stops</th>\n",
       "      <th>equipment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2B</td>\n",
       "      <td>410</td>\n",
       "      <td>AER</td>\n",
       "      <td>2965</td>\n",
       "      <td>KZN</td>\n",
       "      <td>2990</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>CR2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2B</td>\n",
       "      <td>410</td>\n",
       "      <td>ASF</td>\n",
       "      <td>2966</td>\n",
       "      <td>KZN</td>\n",
       "      <td>2990</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>CR2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2B</td>\n",
       "      <td>410</td>\n",
       "      <td>ASF</td>\n",
       "      <td>2966</td>\n",
       "      <td>MRV</td>\n",
       "      <td>2962</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>CR2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2B</td>\n",
       "      <td>410</td>\n",
       "      <td>CEK</td>\n",
       "      <td>2968</td>\n",
       "      <td>KZN</td>\n",
       "      <td>2990</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>CR2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2B</td>\n",
       "      <td>410</td>\n",
       "      <td>CEK</td>\n",
       "      <td>2968</td>\n",
       "      <td>OVB</td>\n",
       "      <td>4078</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>CR2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  airline airline_id source_airport source_airport_id destination_airport  \\\n",
       "0      2B        410            AER              2965                 KZN   \n",
       "1      2B        410            ASF              2966                 KZN   \n",
       "2      2B        410            ASF              2966                 MRV   \n",
       "3      2B        410            CEK              2968                 KZN   \n",
       "4      2B        410            CEK              2968                 OVB   \n",
       "\n",
       "  destination_airport_id codeshare  stops equipment  \n",
       "0                   2990       NaN      0       CR2  \n",
       "1                   2990       NaN      0       CR2  \n",
       "2                   2962       NaN      0       CR2  \n",
       "3                   2990       NaN      0       CR2  \n",
       "4                   4078       NaN      0       CR2  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flight_routes_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert the data from SAS to Parquet\n",
    "All subsequent work in this project will be done in Spark. In this step, we read in the SAS file and save it in Parquet in the sas_data folder in the project workspace. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.\\\n",
    "config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11\")\\\n",
    ".enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spark=spark.read.format('com.github.saurfang.sas.spark').load('../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3096313"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_spark.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spark.write.parquet(\"sas_data\", mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "Identify data quality issues, like missing values, duplicate data, etc.\n",
    "\n",
    "1. Many columns in the immigration data set are sparsely populated. As can be seen from the output of the count on April 2016 dataset, there are many null values in columns insnum, occup, entdepu. We will drop these columns and some other columns that are not required in the analysis tables from the dataset. \n",
    "2. Remove null values from columns of interest. \n",
    "\n",
    "#### Cleaning Steps\n",
    "Document steps necessary to clean the data\n",
    "1. Drop duplicate rows from dataset if any. \n",
    "2. From immigration data set, drop columns not relevant for analysis e.g. insnum, occup etc.  \n",
    "3. For the immigration dataset drop the rows with nulls in columns are relevant for analyis.  \n",
    "4. For airport data set, only consider records with non null iata_code. \n",
    "5. Weather dataset has records from 1700. Filter out the dataset to only contain records from yhear 2000 onwards. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "immigration_df=spark.read.parquet(\"sas_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing cleaning tasks here\n",
    "immigration_df = immigration_df.dropDuplicates()\n",
    "immigration_df = immigration_df.drop('insnum', 'occup', 'entdepa', 'entdepd', 'entdepu')\n",
    "# drop rows which have null values in columns relevant for analysis. \n",
    "immigration_df = immigration_df.na.drop(how='any', thresh=None, \n",
    "                             subset=['i94mode', 'i94addr', 'i94bir', 'i94cit', 'i94res', 'i94visa', 'biryear', 'gender', 'airline', 'fltno', 'visatype']\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "airport_codes_df = spark.read.format(\"csv\")\\\n",
    "                      .option(\"header\", \"true\")\\\n",
    "                      .option(\"inferSchema\", \"true\")\\\n",
    "                      .load(\"/home/workspace/airport-codes_csv.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "airport_codes_df = airport_codes_df.dropna(how='any', subset=['iata_code'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "flight_routes_df = spark.read.format(\"csv\")\\\n",
    "                      .option(\"header\", \"true\")\\\n",
    "                      .option(\"inferSchema\", \"true\")\\\n",
    "                      .load(\"/home/workspace/routes.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67663"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flight_routes_df.dropDuplicates()\n",
    "flight_routes_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['airline',\n",
       " 'airline_id',\n",
       " 'source_airport',\n",
       " 'source_airport_id',\n",
       " 'destination_airport',\n",
       " 'destination_airport_id',\n",
       " 'codeshare',\n",
       " 'stops',\n",
       " 'equipment']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flight_routes_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "demographics_df = spark.read.format(\"csv\")\\\n",
    "                     .option(\"sep\", \";\")\\\n",
    "                     .option(\"header\", \"true\")\\\n",
    "                     .option(\"inferSchema\", \"true\")\\\n",
    "                     .load(\"/home/workspace/us-cities-demographics.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "demographics_df = demographics_df.sort(demographics_df['State Code'], demographics_df['Race'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "demographics_df = demographics_df.withColumnRenamed('State', 'state')\\\n",
    "                                 .withColumnRenamed('State Code', 'state_code')\\\n",
    "                                 .withColumnRenamed('City', 'city')\\\n",
    "                                 .withColumnRenamed('Median Age', 'median_age')\\\n",
    "                                 .withColumnRenamed('Male Population', 'male_population')\\\n",
    "                                 .withColumnRenamed('Female Population', 'female_population')\\\n",
    "                                 .withColumnRenamed('Total Population', 'total_population')\\\n",
    "                                 .withColumnRenamed('Number of Veterans', 'number_of_veterans')\\\n",
    "                                 .withColumnRenamed('Foreign-born', 'foreign_born')\\\n",
    "                                 .withColumnRenamed('Average Household Size', 'average_household_size')\\\n",
    "                                 .withColumnRenamed('Race', 'race')\\\n",
    "                                 .withColumnRenamed('Count', 'count')                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(city='Anchorage', state='Alaska', median_age=32.2, male_population=152945, female_population=145750, total_population=298695, number_of_veterans=27492, foreign_born=33258, average_household_size=2.77, state_code='AK', race='American Indian and Alaska Native', count=36339)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demographics_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df = spark.read.format(\"csv\")\\\n",
    "                .option(\"header\", \"true\")\\\n",
    "                .option(\"inferSchema\", \"true\")\\\n",
    "                .load(\"../../data2/GlobalLandTemperaturesByCity.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8599212"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df = weather_df.filter(year(weather_df.dt)>=2000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "579150"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df = weather_df.withColumnRenamed('dt', 'dt')\\\n",
    "                       .withColumnRenamed('AverageTemperature', 'average_temperature')\\\n",
    "                       .withColumnRenamed('AverageTemperatureUncertainty', 'average_temperature_uncertainty')\\\n",
    "                       .withColumnRenamed('City', 'city')\\\n",
    "                       .withColumnRenamed('Country', 'country')\\\n",
    "                       .withColumnRenamed('Latitude', 'latitude')\\\n",
    "                       .withColumnRenamed('Longitude', 'longitude')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(dt=datetime.datetime(2000, 1, 1, 0, 0), average_temperature=3.065, average_temperature_uncertainty=0.37200000000000005, city='Ã…rhus', country='Denmark', latitude='57.05N', longitude='10.33E')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "ports_df = spark.read.format(\"csv\")\\\n",
    "                    .option(\"header\", \"true\")\\\n",
    "                    .option(\"inferSchema\", \"true\")\\\n",
    "                    .load(\"/home/workspace/port_mappings.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "trim = udf(lambda c: c.strip())\n",
    "ports_df = ports_df.withColumn('airport_code', trim(ports_df['airport_code'])) \\\n",
    "                    .withColumn('city', trim(ports_df['city'])) \\\n",
    "                    .withColumn('state', trim(ports_df['state']))\n",
    "ports_df = ports_df.withColumnRenamed('airport_code', 'port_of_entry_code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+-----+\n",
      "|port_of_entry_code|                city|state|\n",
      "+------------------+--------------------+-----+\n",
      "|               ALC|               ALCAN|   AK|\n",
      "|               ANC|           ANCHORAGE|   AK|\n",
      "|               BAR|BAKER AAF - BAKER...|   AK|\n",
      "|               DAC|       DALTONS CACHE|   AK|\n",
      "|               PIZ|DEW STATION PT LA...|   AK|\n",
      "|               DTH|        DUTCH HARBOR|   AK|\n",
      "|               EGL|               EAGLE|   AK|\n",
      "|               FRB|           FAIRBANKS|   AK|\n",
      "|               HOM|               HOMER|   AK|\n",
      "|               HYD|               HYDER|   AK|\n",
      "|               JUN|              JUNEAU|   AK|\n",
      "|               5KE|           KETCHIKAN|   AK|\n",
      "|               KET|           KETCHIKAN|   AK|\n",
      "|               MOS|MOSES POINT INTER...|   AK|\n",
      "|               NIK|             NIKISKI|   AK|\n",
      "|               NOM|                 NOM|   AK|\n",
      "|               PKC|         POKER CREEK|   AK|\n",
      "|               ORI|      PORT LIONS SPB|   AK|\n",
      "|               SKA|             SKAGWAY|   AK|\n",
      "|               SNP|     ST. PAUL ISLAND|   AK|\n",
      "+------------------+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ports_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries_df=spark.read.format(\"csv\")\\\n",
    "                    .option(\"header\", \"true\")\\\n",
    "                    .option(\"inferSchema\", \"true\")\\\n",
    "                    .load(\"/home/workspace/countries.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(country_code=582, country='MEXICO Air Sea and Not Reported (I-94 no land arrivals)'),\n",
       " Row(country_code=236, country='AFGHANISTAN'),\n",
       " Row(country_code=101, country='ALBANIA'),\n",
       " Row(country_code=316, country='ALGERIA'),\n",
       " Row(country_code=102, country='ANDORRA'),\n",
       " Row(country_code=324, country='ANGOLA')]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countries_df.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = '/home/workspace/output/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "flight_routes_df.write.csv(os.path.join(output_path, 'flight_routes'), mode='overwrite', header=True)\n",
    "demographics_df.write.csv(os.path.join(output_path, 'demographics_data'), mode='overwrite', header=True) \n",
    "weather_df.write.csv(os.path.join(output_path, 'weather_data'), mode='overwrite', header=True) \n",
    "ports_df.write.csv(os.path.join(output_path, 'ports_data'), mode='overwrite', header=True) \n",
    "countries_df.write.csv(os.path.join(output_path, 'countries_data'), mode='overwrite', header=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "Map out the conceptual data model and explain why you chose that model\n",
    "\n",
    "<img src=\"dend_capstone_data_model-analytics2.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.1.1 Analytical Tables \n",
    "1. immigration_data_with_demographics\n",
    "Data is stored in output/immigration_demographics/ folder. It is partitioned by immigration year and immigration month.  The data is formed by a join between immigration data and the grouped state demographics. This table can be used to run queries on immigration data to compare with demographic information in a state. \n",
    "\n",
    "2. immigration_analysis_based_on_airlines\n",
    "Data is stored in output/immigration_airlines folder. It is partitioned by immigration year and immigration month. This data is formed by joining immigration dataset with a data set of airlines and flights to US from countries outside US. This dataset can be of use in analyzing the countries and cities from where visitors travel to US. \n",
    "\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "The data will be pipelined into a datalake using ELT approach\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all the needed datasets \n",
    "demographics_data_df = spark.read.format(\"csv\")\\\n",
    "                .option(\"header\", \"true\")\\\n",
    "                .option(\"inferSchema\", \"true\")\\\n",
    "                .load(os.path.join(output_path, 'demographics_data'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(city='Folsom', state='California', median_age=40.9, male_population=41051, female_population=35317, total_population=76368, number_of_veterans=4187, foreign_born=13234, average_household_size=2.62, state_code='CA', race='American Indian and Alaska Native', count=998)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demographics_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "demographics_df_by_state = demographics_data_df.groupBy('state_code').agg(\n",
    "    avg('median_age'),\n",
    "    sum('male_population'), sum('female_population'), sum('total_population'),\n",
    "    sum('number_of_veterans'), sum('foreign_born'), avg('average_household_size'), sum('count')                                                           \n",
    ").sort(\n",
    "    'state_code'\n",
    ")\n",
    "demographics_df_by_state = demographics_df_by_state.select(\n",
    "     demographics_df_by_state['state_code'],\n",
    "     demographics_df_by_state['avg(median_age)'].alias('average_median_age'), \n",
    "     demographics_df_by_state['sum(male_population)'].alias('total_male_population'),\n",
    "     demographics_df_by_state['sum(female_population)'].alias('total_female_population'),\n",
    "     demographics_df_by_state['sum(total_population)'].alias('total_population'),\n",
    "     demographics_df_by_state['sum(number_of_veterans)'].alias('total_number_of_veterans'),\n",
    "     demographics_df_by_state['sum(foreign_born)'].alias('total_foreign_born'),\n",
    "     demographics_df_by_state['avg(average_household_size)'].alias('average_household_size'),\n",
    "     demographics_df_by_state['sum(count)'].alias('total_count_by_race')   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(state_code='AK', average_median_age=32.2, total_male_population=764725, total_female_population=728750, total_population=1493475, total_number_of_veterans=137460, total_foreign_born=166290, average_household_size=2.77, total_count_by_race=336228)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demographics_df_by_state.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UDF for converting SAS numeric dates to ISO date format of YYYY-MM-DD\n",
    "def convert_sasdate(sas_date):\n",
    "    try:\n",
    "        obj_date = date(1960,1,1) + timedelta(days=sas_date)\n",
    "        return obj_date.isoformat()\n",
    "    except:\n",
    "        return '' \n",
    "convert_date_udf=udf(convert_sasdate, StringType())   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UDF for getting country name from country codes in I94res or I94port tables\n",
    "def get_country_name(country_code_from_i94_record):\n",
    "    try:\n",
    "        #return countries_df.filter(countries_df.country_code==country_code_from_i94_record).first().country\n",
    "        return countries_dict.get(country_code_from_i94_record)\n",
    "    except:\n",
    "        return ''\n",
    "get_country_name_udf = udf(get_country_name, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries_list = countries_df.rdd.map(lambda row: row.asDict()).collect()\n",
    "countries_dict = {}\n",
    "for cc in countries_list:\n",
    "    countries_dict[cc['country_code']]=cc['country']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "immigration_df = immigration_df.withColumn('cicid', immigration_df['cicid'].cast(LongType())) \\\n",
    "                       .withColumn('I94yr', immigration_df['i94yr'].cast(IntegerType())) \\\n",
    "                       .withColumn('I94mon', immigration_df['i94mon'].cast(IntegerType())) \\\n",
    "                       .withColumn('I94cit', immigration_df['i94cit'].cast(IntegerType())) \\\n",
    "                       .withColumn('I94res', immigration_df['i94res'].cast(IntegerType())) \\\n",
    "                       .withColumnRenamed('i94port', 'I94port') \\\n",
    "                       .withColumn('I94mode', immigration_df['i94mode'].cast(IntegerType())) \\\n",
    "                       .withColumn('I94bir', immigration_df['i94bir'].cast(IntegerType())) \\\n",
    "                       .withColumn('biryear', immigration_df['biryear'].cast(IntegerType())) \\\n",
    "                       .withColumn('I94visa', immigration_df['i94visa'].cast(IntegerType())) \\\n",
    "                       .withColumn('admnum', immigration_df['admnum'].cast(LongType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(cicid=1508, I94yr=2016, I94mon=4, I94cit=104, I94res=104, I94port='NYC', arrdate=20545.0, I94mode=1, i94addr='NY', depdate=20552.0, I94bir=16, I94visa=2, count=1.0, dtadfile='20160401', visapost=None, matflag='M', biryear=2000, dtaddto='06292016', gender='F', airline='LX', admnum=55416411533, fltno='00016', visatype='WT')"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "immigration_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "immigration_df = immigration_df.withColumn('I94res', get_country_name_udf(immigration_df['I94res']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "immigration_df =  immigration_df.withColumn('I94cit', get_country_name_udf(immigration_df['I94cit']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2492328"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "immigration_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build the table immigration_data_with_demographics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "immigration_data_with_demographics=immigration_df.select(\n",
    "    'cicid', 'I94yr', 'I94mon', 'I94cit', 'I94res', 'I94port', 'i94addr', 'I94bir', 'gender'\n",
    ").join(\n",
    "    demographics_df_by_state, \n",
    "    immigration_df.i94addr==demographics_df_by_state.state_code\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "immigration_data_with_demographics = immigration_data_with_demographics.select(\n",
    "    immigration_data_with_demographics.cicid.alias('immigration_id'),\n",
    "    immigration_data_with_demographics.I94yr.alias('immigration_year'),\n",
    "    immigration_data_with_demographics.I94mon.alias('immigration_month'), \n",
    "    immigration_data_with_demographics.I94cit.alias('citizenship'),\n",
    "    immigration_data_with_demographics.I94res.alias('country_of_residence'), \n",
    "    immigration_data_with_demographics.I94port.alias('port_of_entry'),\n",
    "    immigration_data_with_demographics.i94addr.alias('destination_state_on_i94'), \n",
    "    immigration_data_with_demographics.I94bir.alias('age'), \n",
    "    immigration_data_with_demographics.gender, \n",
    "    immigration_data_with_demographics.state_code,\n",
    "    immigration_data_with_demographics.average_median_age,\n",
    "    immigration_data_with_demographics.total_male_population,\n",
    "    immigration_data_with_demographics.total_female_population,\n",
    "    immigration_data_with_demographics.total_population,\n",
    "    immigration_data_with_demographics.total_number_of_veterans,\n",
    "    immigration_data_with_demographics.total_foreign_born, \n",
    "    immigration_data_with_demographics.average_household_size,\n",
    "    immigration_data_with_demographics.total_count_by_race\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(immigration_id=1508, immigration_year=2016, immigration_month=4, citizenship='BELGIUM', country_of_residence='BELGIUM', port_of_entry='NYC', destination_state_on_i94='NY', age=16, gender='F', state_code='NY', average_median_age=35.57037037037038, total_male_population=23422799, total_female_population=25579256, total_population=49002055, total_number_of_veterans=1019097, total_foreign_born=17186873, average_household_size=2.77037037037037, total_count_by_race=11377068)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "immigration_data_with_demographics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2378241"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "immigration_data_with_demographics.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "immigration_data_with_demographics.write.partitionBy('immigration_year', 'immigration_month').parquet(\n",
    "    os.path.join(output_path, 'immigration_demographics/'), \n",
    "    mode='overwrite'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build the table immigration_data_with_flight_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "immigration_df = immigration_df.withColumn('arrdate', convert_date_udf(immigration_df['arrdate']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "immigration_df = immigration_df.withColumn('depdate', convert_date_udf(immigration_df['depdate']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter airports data only on large airports. The assumption here is that iinternational flights take off from \n",
    "# the airports categorized as large_airport in the dataset. \n",
    "airport_codes_df = airport_codes_df.filter(\n",
    "    airport_codes_df.iata_code.isNotNull()\n",
    ").filter(\n",
    "    airport_codes_df.type == 'large_airport'\n",
    ")\n",
    "# Create a list of airport codes for US airports \n",
    "airports_in_us = airport_codes_df.filter(airport_codes_df.iso_country=='US').select('iata_code').distinct().collect()\n",
    "airports_in_us_list = [row.iata_code for row in airports_in_us]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_airports_outside_us = airport_codes_df.filter(airport_codes_df.iso_country != 'US')\n",
    "flights_from_large_airports_outside_us = large_airports_outside_us.join(\n",
    "    flight_routes_df, \n",
    "    large_airports_outside_us.iata_code==flight_routes_df.source_airport\n",
    ")\n",
    "flights_to_airports_in_us=flights_from_large_airports_outside_us.where(\n",
    "    flights_from_large_airports_outside_us.destination_airport.isin(airports_in_us_list)\n",
    ")\n",
    "flights_to_airports_in_us = flights_to_airports_in_us.select(\n",
    "            'iata_code',  'name', 'iso_country', 'iso_region', 'municipality',\n",
    "            flights_to_airports_in_us.airline.alias('airline_for_flights_to_us'), \n",
    "            'source_airport', \n",
    "            'destination_airport'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "immigration_analysis_based_on_airlines = immigration_df.select(\n",
    "    'cicid', 'I94yr', 'I94mon', 'I94cit', 'I94res', 'I94port', 'I94bir',  'gender', 'arrdate', 'depdate', 'airline', 'fltno'\n",
    ").join(\n",
    "    flights_to_airports_in_us, \n",
    "    immigration_df.airline==flights_to_airports_in_us.airline_for_flights_to_us\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "immigration_analysis_based_on_airlines = immigration_analysis_based_on_airlines.select(\n",
    "    immigration_analysis_based_on_airlines.cicid.alias('immigration_id'),\n",
    "    immigration_analysis_based_on_airlines.I94yr.alias('immigration_year'),\n",
    "    immigration_analysis_based_on_airlines.I94mon.alias('immigration_month'), \n",
    "    immigration_analysis_based_on_airlines.I94cit.alias('citizenship'),\n",
    "    immigration_analysis_based_on_airlines.I94res.alias('country_of_residence'), \n",
    "    immigration_analysis_based_on_airlines.I94port.alias('port_of_entry'),\n",
    "    immigration_analysis_based_on_airlines.gender,\n",
    "    immigration_analysis_based_on_airlines.arrdate.alias('arrival_date'),\n",
    "    immigration_analysis_based_on_airlines.depdate.alias('departure_date'), \n",
    "    immigration_analysis_based_on_airlines.airline, \n",
    "    immigration_analysis_based_on_airlines.fltno.alias('flight_number'),\n",
    "    immigration_analysis_based_on_airlines.iata_code.alias('iata_code_origination_airport'),  \n",
    "    immigration_analysis_based_on_airlines.name.alias('origination_airport_name'), \n",
    "    immigration_analysis_based_on_airlines.iso_country.alias('origination_airport_country'), \n",
    "    immigration_analysis_based_on_airlines.iso_region.alias('origination_airport_region'), \n",
    "    immigration_analysis_based_on_airlines.municipality.alias('origination_airport_municipality'), \n",
    "    immigration_analysis_based_on_airlines.destination_airport.alias('iata_code_destination_airport')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(immigration_id=52676, immigration_year=2016, immigration_month=4, citizenship='INDONESIA', country_of_residence='INDONESIA', port_of_entry='LOS', gender='M', arrival_date='2016-04-01', departure_date='2016-06-06', airline='CI', flight_number='00006', iata_code_origination_airport='GUA', origination_airport_name='La Aurora Airport', origination_airport_country='GT', origination_airport_region='GT-GU', origination_airport_municipality='Guatemala City', iata_code_destination_airport='LAX'),\n",
       " Row(immigration_id=52676, immigration_year=2016, immigration_month=4, citizenship='INDONESIA', country_of_residence='INDONESIA', port_of_entry='LOS', gender='M', arrival_date='2016-04-01', departure_date='2016-06-06', airline='CI', flight_number='00006', iata_code_origination_airport='TPE', origination_airport_name='Taiwan Taoyuan International Airport', origination_airport_country='TW', origination_airport_region='TW-TAO', origination_airport_municipality='Taipei', iata_code_destination_airport='SFO'),\n",
       " Row(immigration_id=52676, immigration_year=2016, immigration_month=4, citizenship='INDONESIA', country_of_residence='INDONESIA', port_of_entry='LOS', gender='M', arrival_date='2016-04-01', departure_date='2016-06-06', airline='CI', flight_number='00006', iata_code_origination_airport='TPE', origination_airport_name='Taiwan Taoyuan International Airport', origination_airport_country='TW', origination_airport_region='TW-TAO', origination_airport_municipality='Taipei', iata_code_destination_airport='LAX'),\n",
       " Row(immigration_id=52676, immigration_year=2016, immigration_month=4, citizenship='INDONESIA', country_of_residence='INDONESIA', port_of_entry='LOS', gender='M', arrival_date='2016-04-01', departure_date='2016-06-06', airline='CI', flight_number='00006', iata_code_origination_airport='TPE', origination_airport_name='Taiwan Taoyuan International Airport', origination_airport_country='TW', origination_airport_region='TW-TAO', origination_airport_municipality='Taipei', iata_code_destination_airport='HNL'),\n",
       " Row(immigration_id=52676, immigration_year=2016, immigration_month=4, citizenship='INDONESIA', country_of_residence='INDONESIA', port_of_entry='LOS', gender='M', arrival_date='2016-04-01', departure_date='2016-06-06', airline='CI', flight_number='00006', iata_code_origination_airport='NRT', origination_airport_name='Narita International Airport', origination_airport_country='JP', origination_airport_region='JP-12', origination_airport_municipality='Tokyo', iata_code_destination_airport='HNL'),\n",
       " Row(immigration_id=52676, immigration_year=2016, immigration_month=4, citizenship='INDONESIA', country_of_residence='INDONESIA', port_of_entry='LOS', gender='M', arrival_date='2016-04-01', departure_date='2016-06-06', airline='CI', flight_number='00006', iata_code_origination_airport='KIX', origination_airport_name='Kansai International Airport', origination_airport_country='JP', origination_airport_region='JP-27', origination_airport_municipality='Osaka', iata_code_destination_airport='JFK')]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "immigration_analysis_based_on_airlines.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "immigration_analysis_based_on_airlines.write.partitionBy('immigration_year', 'immigration_month').parquet(\n",
    "    os.path.join(output_path, 'immigration_airlines/'), \n",
    "    mode='overwrite'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "218944973"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "immigration_analysis_based_on_airlines.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform quality checks here\n",
    "# check that the dataset PK is unique\n",
    "assert immigration_data_with_demographics.count() == immigration_data_with_demographics.select('immigration_id').distinct().count()\n",
    "# check that essential fields like state code have valid values\n",
    "assert immigration_data_with_demographics.filter(immigration_data_with_demographics.state_code.isNull()).count() == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Data dictionary \n",
    "Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file.\n",
    "\n",
    "| Column        | Purpose                              | Source         | \n",
    "|---------------|--------------------------------------|-----------------|\n",
    "|immigration_id | unique identifier for the record, is a PK in the dataset|maps to cicid column in immigration_data |\n",
    "|immigration_year| year in which I94 was issued  | maps to i94yr in immigration_data | \n",
    "|immigration_month| month in which I94 was issued  | maps to i94mon in immigration_data | \n",
    "|citizenship| Visitor's country of citizenship| maps to i94cit in immigration_data. It is transformed in a country name based on the countries mapping given in I94_SAS_Labels_Descriptions| \n",
    "|country_of_residence| Visitor's country of residence| maps to i94res in immigration_data. It is transformed in a country name based on the countries mapping given in I94_SAS_Labels_Descriptions | \n",
    "|port_of_entry| 3 character city code where the I94 is issued| maps to i94port in immigration_data | \n",
    "|address_on_i94| 2 character state code corresponding to the visitor's address in US during their stay| maps to i94addr in immigration_data | \n",
    "|age| visitor's age| maps to i94bir in immigration_data | \n",
    "|gender| visitor's gender| maps to gender in immigration_data | \n",
    "|state_code| 2 character state code for demographics | maps to state code in demographics data | \n",
    "|average_median_age| This is the median age in a state | obtained by grouping demographics data for a state and taking an average of median age | \n",
    "|total_male_population| Total male population in a state | obtained by grouping demographics data for a state and totaling the male population| \n",
    "|total_female_population| Total female population in a state | obtained by grouping demographics data for a state and totaling the female population| \n",
    "|total_population|Total population in a state | obtained by grouping demographics data for a state and totaling the population| \n",
    "|total_number_of_veterans|Total number of veterans in a state | obtained by grouping demographics data for a state and totaling the number of veterans column| \n",
    "|total_foreign_born|Total number of foreign born people in a state | obtained by grouping demographics data for a state and totaling the numbers in foreign_born | \n",
    "|average_household_size|Average Household size in a state | obtained by grouping demographics data for a state and taking an average of average_household_size |\n",
    "|total_count_by_race|Total number of people listed by race in demographics | obtained by grouping demographics data for a state and totaling the count of people of different races | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Complete Project Write Up\n",
    " \n",
    "#### Rationale of Tools \n",
    "I used Spark for performing the ETL on the various datasets and I stored the resultant analytical tables on S3. This breakdown helped with separating the compute and storage. I could have used a Data Warehouse like Redshift to store the data but then that would have made scaling complex and costly. \n",
    "\n",
    "The decision to use Spark came from the need to process large datasets. Spark SQL makes it easy to run SQL like queries and operations on Dataframes. Spark works well with S3 as it can use cluster processing to pull partiotioned data from S3 folders. \n",
    "\n",
    "I decided to build my analytics database as a data lake on S3 because of the following reasons - \n",
    "- Scalability and Availability - S3 is infinitely scalable and provides continous availability. \n",
    "- Storage Data Costs  - S3 storage is very cheap and large amounts of data can be stored without substantial increase in costs.\n",
    "- Scema on read - I can store data in flexible forms without confining the data into predefined tables. \n",
    "- Data on S3 is easily integrated with data warehouses and ML libraries. Storing the data on S3 does not prevent me from doing more powerful analytics or ML training as S3 is widely supported by libraries, frameworks and data warehouses. \n",
    "\n",
    "#### Data Update Frequency \n",
    "The data can be updated on a weekly basis. Monthly data sets provided as examples were a few 100 GBs in size. Doing a weekly ETL will keep the data fresh and the ETL processes will run quickly and the analytics data lakes will update frequently. If the analytics are needed on a more real time basis, the ETL pipeline can be run once daily. \n",
    "\n",
    "#### Scenarios \n",
    "##### Data increased by 100x \n",
    "As I have built the ETL in Spark, an increase in data can be easily handled by the size of the EMR cluster that will run the Spark job to perform ETL. \n",
    "\n",
    "##### Data populates a dashboard that must be updated on a daily basis by 7am every day \n",
    "For a more frequent requirement, the ETL process will need to run overnight. This ETL process can be modeled into a DAG of tasks in a workflow scheduler like Airflow.  The DAG can then be scheduled to run overnight. There can be two DAGs - \n",
    "- first one will populates the data lake in S3 and \n",
    "- the second DAG will trigger after the data population is completed. It will run an ELT pipeline on the data lake to load the data in a data warehouse like Amazon Redshift. There can be additional DAGs or tasks in the same DAG to transform the data to the format required for the dashboard. \n",
    "Both the DAGs will be scheduled to run on a daily frequency. \n",
    "\n",
    "##### The database needed to be accessed by 100+ people\n",
    "If the number of users interested in analysing the data increases to 100+, it would be best to store the data in a highly distributed and scalable database like Cassandra. Dependending on the use cases of users, the data can be ELTed from the data lake to Cassandra in the form desired by users or the data can be stored directly on Cassandra. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
